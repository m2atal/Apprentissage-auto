{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import typing\n",
    "import yaml\n",
    "\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation steps\n",
    "1. Scaled Dot-product attention\n",
    "2. Attention Head\n",
    "3. Multi-Head attention mechanism\n",
    "4. Positional encoding\n",
    "5. Feed-forward block\n",
    "6. Encoder\n",
    "7. Decoder\n",
    "8. Transformer\n",
    "\n",
    "**Note** that we'll try to produce clean commented code and use the `typing` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scaled Dot-product attention\n",
    "\n",
    "It consists of a simple attention mechanism that is a dot-product between the **Query** (data being processed)\n",
    "and the **Key** (hidden state of the encoder).\n",
    "\n",
    "\n",
    "As the value of the dot product grows with the dimensionality of the Query and Key vectors\n",
    "we need to rescale the dot product to prevent it from exploding into huge values.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    :param query: a Tensor\n",
    "    :param key: a Tensor\n",
    "    :param value: a Tensor\n",
    "    \n",
    "    :return: a Tensor that is the softmax of the scaled dot-product\n",
    "    \"\"\"\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention Head\n",
    "\n",
    "A class that inherits from torch.nn.module.\n",
    "the constructor takes as parameters:\n",
    "- the input dimension `dim_in` as an `int` \n",
    "- an output dimension `dim_k` as an `int` for the Query and the Key\n",
    "- an output dimension `dim_v` as an `int` for the Value\n",
    "\n",
    "The forward method takes as parameters:\n",
    "- the `query` as a Tensor\n",
    "- the `key` as a Tensor\n",
    "- the `value` as a Tensor\n",
    "\n",
    "And return the scaled dot-product as a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head attention mechanism\n",
    "\n",
    "A class that inherits from `torch.nn.module` and that combines a number `num_heads` of Attention Heads \n",
    "before combining the results and apply a `Linear` layer.\n",
    "\n",
    "The constructor takes as parameters:\n",
    "- the number of heads as an `int` (num_heads)\n",
    "- the input dimension `dim_in` as an `int` \n",
    "- an output dimension `dim_k` as an `int` for the Query and the Key\n",
    "- an output dimension `dim_v` as an `int` for the Value\n",
    "\n",
    "The `Linear` layer takes as input a concatenation of the attention vectors and output a vector of \n",
    "dimension `dim_in`.\n",
    "\n",
    "The forward method takes as parameters:\n",
    "- the `query` as a Tensor\n",
    "- the `key` as a Tensor\n",
    "- the `value` as a Tensor\n",
    "\n",
    "And return the result of the multi-head attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        # TO DO\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Positional encoding\n",
    "\n",
    "As the attention mechanism is not aware of any sequence order, this information is \n",
    "provided to the network explicitly by encoding the position of each element in the sequence.\n",
    "\n",
    "The positional encodings have the same dimension $d_{model}$ as the embeddings\n",
    "\n",
    "There are different ways to encode this information. In the original paper, the authors propose to use the \n",
    "following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathrm{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^\\frac{2i}{d_{model}}}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathrm{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^\\frac{2i}{d_{model}}}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional \n",
    "encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\Pi$ to $10000 \\cdot 2\\Pi$.\n",
    "\n",
    "We implement the position encoding with a method that takes as input:\n",
    "- an integer `seq_len` that is the length of the sequence\n",
    "- an integer `dim_model` that is the dimension of the \n",
    "- the device, `torch.device` used by PyTorch\n",
    "\n",
    "And return a `Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\")) -> Tensor:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = (pos / 1e4) ** (dim // dim_model)\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Residual block\n",
    "\n",
    "`We employ a residual connection around each ofthe two sub-layers, followed by layer normalization.  That is, the output of each sub-layer is LayerNorm(x+ Sublayer(x)), where Sublayer(x)is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension model= 512`\n",
    "\n",
    "The two **sub-layers** are implemted as a function that returns a `torch.nn.sequential` composed of:\n",
    "- one linear layer ($dim\\_input \\times dim\\_feedforward$)\n",
    "- one `ReLU`activation\n",
    "- one linear layer ($dim\\_feedforward \\times dim\\_input$)\n",
    "\n",
    "The dimensionality of input and output is 512, and the inner-layer has dimensionality 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. â€¦ We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.`\n",
    "\n",
    "Now we implement the Residual block in a `class` that inherits from the `torch.nn.module`class.\n",
    "\n",
    "The constructor of the class takes as input:\n",
    "- `sublayer` a feed-forward module of type `torch.nn.module`\n",
    "- `dimension`, an integer that is the size of the output of the multi-head attention\n",
    "- a probability `float` of dropout (by default: 0.1)\n",
    "\n",
    "The forward method of the Residual module takes as input a sequence/collection of `Tensors` of undefined length.\n",
    "We assume that the **value** `Tensor` will be the last to be complient with the signature of the \n",
    "`MultiHeadAttention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, sublayer: torch.nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TO DO\n",
    "\n",
    "    def forward(self, *tensors: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoder\n",
    "\n",
    "Now we can create the `TransformerEncoderLayer` that will be duplicated to create the Encoder (left side of the network).\n",
    "\n",
    "The constructor of the class takes as input:\n",
    "- an integer `dim_model` that gives the dimension of the embeddings (concatenated ouptut of the multi-head attention mechanism (default is 512)\n",
    "- an integer `num_heads` that gives the number of attention heads (default is 8)\n",
    "- an integer `dim_feedforward` that gives the dimension of the inner-layer of the feed-forward block\n",
    "- the probability of dropout given as a float `dropout` which default value is 0.1\n",
    "\n",
    "The `TransformerEncoderLayer` consists of one `Residual` block encapsulating a `MultiHeadAttention` followed\n",
    "by one `Residual` block encapsulating a `feed_forward`.\n",
    "\n",
    "The `forward` method takes a single `Tensor`, `src`, as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        dim_k = dim_v = dim_model // num_heads\n",
    "        \n",
    "        # TO DO\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        # TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the complete Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(torch.nn.Module):\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Decoder\n",
    "\n",
    "Creates the class for the Decoder, the right part of the network.\n",
    "\n",
    "First the `TransformerDecoderLayer` that will be duplicated.\n",
    "\n",
    "The constructor takes as inputs:\n",
    "- an integer `dim_model` that gives the dimension of the embeddings (concatenated ouptut of the multi-head attention mechanism (default is 512)\n",
    "- an integer `num_heads` that gives the number of attention heads (default is 8)\n",
    "- an integer `dim_feedforward` that gives the dimension of the inner-layer of the feed-forward block, default is 2048\n",
    "- the probability of dropout given as a float `dropout` which default value is 0.1\n",
    "\n",
    "The `TransformerDecoderLayer` consists of two consecutive blocks `Residual` block encapsulating a `MultiHeadAttention` followed by one `Residual` block encapsulating a `feed_forward`.\n",
    "\n",
    "The `forward` method takes two `Tensor`, `tgt` and `memory` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "    ):\n",
    "        # TO DO\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the `TransfomerDecoder` class that consists of 6 `TransformerDecoderLayer` followed by a linear layer.\n",
    "(don't forget the position encoding) and the final softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "        number_classes=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.linear = torch.nn.Linear(dim_model, number_classes)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
    "        tgt += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "\n",
    "        return torch.softmax(self.linear(tgt), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Transformer\n",
    "\n",
    "Now just put the encoder and decoder together.\n",
    "\n",
    "And put the data through..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "        activation: torch.nn.Module = torch.nn.ReLU(),\n",
    "    ):\n",
    "    # TO DO\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Now Create a Transformers that will process sequences of length 200 vectors of dimension 30.\n",
    "Create a dummy batch of  of input data and check that everything runs smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100, 2])\n"
     ]
    }
   ],
   "source": [
    "src = torch.rand(64, 100, 30)\n",
    "tgt = torch.rand(64, 100, 30)\n",
    "out = Transformer(dim_model=30)(src, tgt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
